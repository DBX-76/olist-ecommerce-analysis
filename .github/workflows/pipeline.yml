name: Full Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    env:
      KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
      KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
      DB_HOST: ${{ secrets.DB_HOST }}
      DB_PORT: ${{ secrets.DB_PORT }}
      DB_NAME: ${{ secrets.DB_NAME }}
      DB_USER: ${{ secrets.DB_USER }}
      DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install kaggle
        run: pip install kaggle

      - name: Configure Kaggle
        run: |
          mkdir -p ~/.kaggle
          echo "{\"username\":\"${{ secrets.KAGGLE_USERNAME }}\",\"key\":\"${{ secrets.KAGGLE_KEY }}\"}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json

      - name: Download dataset from Kaggle
        run: |
          mkdir -p data/raw
          kaggle datasets download -d olistbr/brazilian-ecommerce -p data/raw --unzip

      - name: Create processed data directory
        run: mkdir -p data/processed

      - name: Create report directories
        run: |
          mkdir -p reports/anomaly_detection
          mkdir -p reports/zip_code_reference
          mkdir -p reports/customers
          mkdir -p reports/sellers
            
      - name: Run pipeline
        run: python run_pipeline.py

      - name: Execute notebooks
        run: |
          pip install nbconvert jupyter
          jupyter nbconvert --to notebook --execute notebooks/01_Exploratory_Data_Analysis.ipynb --output 01_Exploratory_Data_Analysis.ipynb
          jupyter nbconvert --to notebook --execute notebooks/02_Data_Quality_Analysis.ipynb --output 02_Data_Quality_Analysis.ipynb
          jupyter nbconvert --to notebook --execute notebooks/03_Customer_Payment_Rank.ipynb --output 03_Customer_Payment_Rank.ipynb
          jupyter nbconvert --to notebook --execute notebooks/04_Customer_Payment_Details.ipynb --output 04_Customer_Payment_Details.ipynb
          jupyter nbconvert --to notebook --execute notebooks/05_Customer_Order_Interval.ipynb --output 05_Customer_Order_Interval.ipynb
      - name: Commit executed notebooks
        run: |
          git config --global user.email "actions@github.com"
          git config --global user.name "GitHub Actions"
          git add notebooks/
          git commit -m "auto: execute notebooks with outputs" || echo "No changes"
          git push

      - name: Upload reports and processed data
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-output
          path: |
            reports/
            data/processed/
