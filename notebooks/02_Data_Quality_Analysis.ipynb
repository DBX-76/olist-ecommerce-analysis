{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Analysis - Brazilian E-Commerce Dataset\n",
    "\n",
    "This notebook focuses on identifying and documenting data quality issues and missing values across all datasets.\n",
    "\n",
    "## Objectives\n",
    "1. Identify missing values in each dataset\n",
    "2. Analyze data types and inconsistencies\n",
    "3. Detect duplicates and outliers\n",
    "4. Document data quality issues\n",
    "5. Create recommendations for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RAW_DATA_PATH = '../data/raw/'\n",
    "REPORTS_PATH = '../reports/'\n",
    "\n",
    "# Ensure reports directory exists\n",
    "os.makedirs(REPORTS_PATH, exist_ok=True)\n",
    "\n",
    "# Define dataset files\n",
    "datasets = {\n",
    "    'orders': 'olist_orders_dataset.csv',\n",
    "    'customers': 'olist_customers_dataset.csv',\n",
    "    'order_items': 'olist_order_items_dataset.csv',\n",
    "    'order_payments': 'olist_order_payments_dataset.csv',\n",
    "    'order_reviews': 'olist_order_reviews_dataset.csv',\n",
    "    'products': 'olist_products_dataset.csv',\n",
    "    'sellers': 'olist_sellers_dataset.csv',\n",
    "    'geolocation': 'olist_geolocation_dataset.csv',\n",
    "    'category_translation': 'product_category_name_translation.csv'\n",
    "}\n",
    "\n",
    "# Load all datasets\n",
    "data = {}\n",
    "\n",
    "for name, filename in datasets.items():\n",
    "    filepath = os.path.join(RAW_DATA_PATH, filename)\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        data[name] = df\n",
    "        print(f\"Loaded {name}: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {name}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal datasets loaded: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze data quality for a single dataset\n",
    "def analyze_data_quality(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Analyze data quality for a dataset.\n",
    "    \n",
    "    Returns a dictionary with quality metrics.\n",
    "    \"\"\"\n",
    "    quality_report = {\n",
    "        'dataset': dataset_name,\n",
    "        'total_rows': df.shape[0],\n",
    "        'total_columns': df.shape[1],\n",
    "        'total_cells': df.shape[0] * df.shape[1],\n",
    "        'duplicate_rows': df.duplicated().sum(),\n",
    "        'duplicate_percentage': round((df.duplicated().sum() / df.shape[0]) * 100, 2),\n",
    "        'memory_usage_mb': round(df.memory_usage(deep=True).sum() / 1024**2, 2)\n",
    "    }\n",
    "    \n",
    "    # Analyze missing values\n",
    "    missing_info = []\n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct = round((missing_count / df.shape[0]) * 100, 2)\n",
    "            missing_info.append({\n",
    "                'column': col,\n",
    "                'missing_count': missing_count,\n",
    "                'missing_percentage': missing_pct,\n",
    "                'data_type': str(df[col].dtype)\n",
    "            })\n",
    "    \n",
    "    quality_report['columns_with_missing'] = len(missing_info)\n",
    "    quality_report['missing_details'] = missing_info\n",
    "    \n",
    "    # Analyze data types\n",
    "    dtype_info = df.dtypes.value_counts().to_dict()\n",
    "    quality_report['data_types'] = dtype_info\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Analyze all datasets\n",
    "all_quality_reports = {}\n",
    "\n",
    "for name, df in data.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    report = analyze_data_quality(df, name)\n",
    "    all_quality_reports[name] = report\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Rows: {report['total_rows']:,}\")\n",
    "    print(f\"Columns: {report['total_columns']}\")\n",
    "    print(f\"Duplicate rows: {report['duplicate_rows']:,} ({report['duplicate_percentage']}%)\")\n",
    "    print(f\"Columns with missing values: {report['columns_with_missing']}\")\n",
    "    print(f\"Memory usage: {report['memory_usage_mb']} MB\")\n",
    "    \n",
    "    # Print missing value details\n",
    "    if report['missing_details']:\n",
    "        print(f\"\\nMissing Values:\")\n",
    "        for item in report['missing_details']:\n",
    "            print(f\"  - {item['column']}: {item['missing_count']:,} ({item['missing_percentage']}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive missing value report\n",
    "missing_report = []\n",
    "\n",
    "for name, df in data.items():\n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct = round((missing_count / df.shape[0]) * 100, 2)\n",
    "            \n",
    "            # Determine severity\n",
    "            if missing_pct > 50:\n",
    "                severity = 'CRITICAL'\n",
    "            elif missing_pct > 20:\n",
    "                severity = 'HIGH'\n",
    "            elif missing_pct > 5:\n",
    "                severity = 'MEDIUM'\n",
    "            else:\n",
    "                severity = 'LOW'\n",
    "            \n",
    "            missing_report.append({\n",
    "                'Dataset': name,\n",
    "                'Column': col,\n",
    "                'Missing_Count': missing_count,\n",
    "                'Missing_Percentage': missing_pct,\n",
    "                'Severity': severity,\n",
    "                'Data_Type': str(df[col].dtype),\n",
    "                'Total_Rows': df.shape[0]\n",
    "            })\n",
    "\n",
    "# Create DataFrame for missing values\n",
    "missing_df = pd.DataFrame(missing_report)\n",
    "\n",
    "if not missing_df.empty:\n",
    "    # Sort by severity and percentage\n",
    "    severity_order = {'CRITICAL': 0, 'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}\n",
    "    missing_df['Severity_Order'] = missing_df['Severity'].map(severity_order)\n",
    "    missing_df = missing_df.sort_values(['Severity_Order', 'Missing_Percentage'], ascending=[True, False])\n",
    "    missing_df = missing_df.drop('Severity_Order', axis=1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE MISSING VALUE REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    missing_df.to_csv(os.path.join(REPORTS_PATH, 'missing_values_report.csv'), index=False)\n",
    "    print(f\"\\nMissing values report saved to: {os.path.join(REPORTS_PATH, 'missing_values_report.csv')}\")\n",
    "else:\n",
    "    print(\"\\nNo missing values found in any dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Type Analysis and Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data types across all datasets\n",
    "dtype_report = []\n",
    "\n",
    "for name, df in data.items():\n",
    "    for col in df.columns:\n",
    "        dtype_report.append({\n",
    "            'Dataset': name,\n",
    "            'Column': col,\n",
    "            'Data_Type': str(df[col].dtype),\n",
    "            'Non_Null_Count': df[col].count(),\n",
    "            'Null_Count': df[col].isnull().sum(),\n",
    "            'Unique_Values': df[col].nunique()\n",
    "        })\n",
    "\n",
    "dtype_df = pd.DataFrame(dtype_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA TYPE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "display(dtype_df)\n",
    "\n",
    "# Save to CSV\n",
    "dtype_df.to_csv(os.path.join(REPORTS_PATH, 'data_types_report.csv'), index=False)\n",
    "print(f\"\\nData types report saved to: {os.path.join(REPORTS_PATH, 'data_types_report.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Date Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and analyze date columns\n",
    "date_columns = []\n",
    "\n",
    "for name, df in data.items():\n",
    "    for col in df.columns:\n",
    "        # Check if column name suggests it's a date\n",
    "        if any(keyword in col.lower() for keyword in ['date', 'time', 'timestamp']):\n",
    "            date_columns.append({\n",
    "                'Dataset': name,\n",
    "                'Column': col,\n",
    "                'Current_Type': str(df[col].dtype),\n",
    "                'Sample_Values': df[col].dropna().head(3).tolist()\n",
    "            })\n",
    "\n",
    "if date_columns:\n",
    "    date_df = pd.DataFrame(date_columns)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATE COLUMNS IDENTIFIED\")\n",
    "    print(\"=\"*80)\n",
    "    display(date_df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    date_df.to_csv(os.path.join(REPORTS_PATH, 'date_columns_report.csv'), index=False)\n",
    "    print(f\"\\nDate columns report saved to: {os.path.join(REPORTS_PATH, 'date_columns_report.csv')}\")\n",
    "else:\n",
    "    print(\"\\nNo date columns identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Duplicate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze duplicates in detail\n",
    "duplicate_report = []\n",
    "\n",
    "for name, df in data.items():\n",
    "    dup_count = df.duplicated().sum()\n",
    "    if dup_count > 0:\n",
    "        duplicate_report.append({\n",
    "            'Dataset': name,\n",
    "            'Duplicate_Rows': dup_count,\n",
    "            'Duplicate_Percentage': round((dup_count / df.shape[0]) * 100, 2),\n",
    "            'Total_Rows': df.shape[0]\n",
    "        })\n",
    "        \n",
    "        # Show sample duplicates\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Duplicates in {name}:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total duplicates: {dup_count:,} ({round((dup_count / df.shape[0]) * 100, 2)}%)\")\n",
    "        print(f\"\\nSample duplicate rows:\")\n",
    "        display(df[df.duplicated(keep=False)].head(6))\n",
    "\n",
    "if duplicate_report:\n",
    "    dup_df = pd.DataFrame(duplicate_report)\n",
    "    dup_df.to_csv(os.path.join(REPORTS_PATH, 'duplicates_report.csv'), index=False)\n",
    "    print(f\"\\nDuplicates report saved to: {os.path.join(REPORTS_PATH, 'duplicates_report.csv')}\")\n",
    "else:\n",
    "    print(\"\\nNo duplicates found in any dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Outlier Detection (Numerical Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in numerical columns using IQR method\n",
    "outlier_report = []\n",
    "\n",
    "for name, df in data.items():\n",
    "    # Select numerical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].notna().sum() > 0:  # Only if column has non-null values\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "            outlier_count = len(outliers)\n",
    "            outlier_pct = round((outlier_count / df[col].notna().sum()) * 100, 2)\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                outlier_report.append({\n",
    "                    'Dataset': name,\n",
    "                    'Column': col,\n",
    "                    'Outlier_Count': outlier_count,\n",
    "                    'Outlier_Percentage': outlier_pct,\n",
    "                    'Lower_Bound': round(lower_bound, 2),\n",
    "                    'Upper_Bound': round(upper_bound, 2),\n",
    "                    'Min_Value': round(df[col].min(), 2),\n",
    "                    'Max_Value': round(df[col].max(), 2)\n",
    "                })\n",
    "\n",
    "if outlier_report:\n",
    "    outlier_df = pd.DataFrame(outlier_report)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OUTLIER DETECTION REPORT (IQR Method)\")\n",
    "    print(\"=\"*80)\n",
    "    display(outlier_df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    outlier_df.to_csv(os.path.join(REPORTS_PATH, 'outliers_report.csv'), index=False)\n",
    "    print(f\"\\nOutliers report saved to: {os.path.join(REPORTS_PATH, 'outliers_report.csv')}\")\n",
    "else:\n",
    "    print(\"\\nNo outliers detected in numerical columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive data quality summary\n",
    "quality_summary = []\n",
    "\n",
    "for name, df in data.items():\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_cells = df.isnull().sum().sum()\n",
    "    missing_pct = round((missing_cells / total_cells) * 100, 2)\n",
    "    \n",
    "    # Count columns with missing values\n",
    "    cols_with_missing = sum(df[col].isnull().sum() > 0 for col in df.columns)\n",
    "    \n",
    "    # Count numerical columns\n",
    "    numeric_cols = len(df.select_dtypes(include=[np.number]).columns)\n",
    "    \n",
    "    # Count categorical columns\n",
    "    categorical_cols = len(df.select_dtypes(include=['object']).columns)\n",
    "    \n",
    "    quality_summary.append({\n",
    "        'Dataset': name,\n",
    "        'Rows': df.shape[0],\n",
    "        'Columns': df.shape[1],\n",
    "        'Numeric_Columns': numeric_cols,\n",
    "        'Categorical_Columns': categorical_cols,\n",
    "        'Missing_Cells': missing_cells,\n",
    "        'Missing_Percentage': missing_pct,\n",
    "        'Columns_With_Missing': cols_with_missing,\n",
    "        'Duplicate_Rows': df.duplicated().sum(),\n",
    "        'Memory_MB': round(df.memory_usage(deep=True).sum() / 1024**2, 2)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(quality_summary)\n",
    "summary_df = summary_df.sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DATA QUALITY SUMMARY DASHBOARD\")\n",
    "print(\"=\"*100)\n",
    "display(summary_df)\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv(os.path.join(REPORTS_PATH, 'data_quality_summary.csv'), index=False)\n",
    "print(f\"\\nData quality summary saved to: {os.path.join(REPORTS_PATH, 'data_quality_summary.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendations for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on data quality analysis\n",
    "recommendations = []\n",
    "\n",
    "for name, df in data.items():\n",
    "    # Check for missing values\n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct = round((missing_count / df.shape[0]) * 100, 2)\n",
    "            \n",
    "            if missing_pct > 50:\n",
    "                action = \"DROP COLUMN - Too many missing values\"\n",
    "                priority = \"HIGH\"\n",
    "            elif missing_pct > 20:\n",
    "                action = \"IMPUTE or DROP ROWS - Consider imputation or removal\"\n",
    "                priority = \"MEDIUM\"\n",
    "            elif missing_pct < 5:\n",
    "                action = \"IMPUTE - Use mean/median/mode or forward fill\"\n",
    "                priority = \"LOW\"\n",
    "            else:\n",
    "                action = \"REVIEW - Investigate cause of missing values\"\n",
    "                priority = \"MEDIUM\"\n",
    "            \n",
    "            recommendations.append({\n",
    "                'Dataset': name,\n",
    "                'Column': col,\n",
    "                'Missing_Percentage': missing_pct,\n",
    "                'Priority': priority,\n",
    "                'Recommended_Action': action\n",
    "            })\n",
    "    \n",
    "    # Check for duplicates\n",
    "    dup_count = df.duplicated().sum()\n",
    "    if dup_count > 0:\n",
    "        recommendations.append({\n",
    "            'Dataset': name,\n",
    "            'Column': 'ALL_COLUMNS',\n",
    "            'Missing_Percentage': 0,\n",
    "            'Priority': 'MEDIUM',\n",
    "            'Recommended_Action': f'REMOVE {dup_count} DUPLICATE ROWS'\n",
    "        })\n",
    "\n",
    "if recommendations:\n",
    "    rec_df = pd.DataFrame(recommendations)\n",
    "    \n",
    "    # Sort by priority\n",
    "    priority_order = {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}\n",
    "    rec_df['Priority_Order'] = rec_df['Priority'].map(priority_order)\n",
    "    rec_df = rec_df.sort_values(['Priority_Order', 'Missing_Percentage'], ascending=[True, False])\n",
    "    rec_df = rec_df.drop('Priority_Order', axis=1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DATA CLEANING RECOMMENDATIONS\")\n",
    "    print(\"=\"*100)\n",
    "    display(rec_df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    rec_df.to_csv(os.path.join(REPORTS_PATH, 'cleaning_recommendations.csv'), index=False)\n",
    "    print(f\"\\nCleaning recommendations saved to: {os.path.join(REPORTS_PATH, 'cleaning_recommendations.csv')}\")\n",
    "else:\n",
    "    print(\"\\nNo data quality issues found. Data is clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"DATA QUALITY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nGenerated Reports:\")\n",
    "print(\"  1. missing_values_report.csv - Detailed missing value analysis\")\n",
    "print(\"  2. data_types_report.csv - Data type information\")\n",
    "print(\"  3. date_columns_report.csv - Date column identification\")\n",
    "print(\"  4. duplicates_report.csv - Duplicate row analysis\")\n",
    "print(\"  5. outliers_report.csv - Outlier detection\")\n",
    "print(\"  6. data_quality_summary.csv - Overall quality dashboard\")\n",
    "print(\"  7. cleaning_recommendations.csv - Actionable recommendations\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Review the cleaning recommendations\")\n",
    "print(\"  2. Prioritize HIGH priority issues\")\n",
    "print(\"  3. Create data cleaning scripts\")\n",
    "print(\"  4. Validate cleaned data\")\n",
    "print(\"  5. Proceed with advanced analysis\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"  - Total datasets analyzed: {len(data)}\")\n",
    "print(f\"  - Total rows across all datasets: {sum(df.shape[0] for df in data.values()):,}\")\n",
    "print(f\"  - Datasets with missing values: {sum(1 for df in data.values() if df.isnull().sum().sum() > 0)}\")\n",
    "print(f\"  - Datasets with duplicates: {sum(1 for df in data.values() if df.duplicated().sum() > 0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}